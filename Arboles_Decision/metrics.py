"""
*** M√©tricas en √°rboles de decisi√≥n ***

El objetivo de este ejercicio es que los estudiantes implementen una funci√≥n que:
Entrene un √°rbol de decisi√≥n usando DecisionTreeClassifier de sklearn.
Haga predicciones en un conjunto de prueba.
Eval√∫e el modelo utilizando m√©tricas como precisi√≥n (accuracy), matriz de confusi√≥n y reporte de clasificaci√≥n.
Pase pruebas unitarias (unittest) que validen el funcionamiento correcto del c√≥digo.


Instrucciones

Implementa una funci√≥n llamada entrenar_y_evaluar_arbol(X_train, y_train, X_test, y_test) que:

- Entrene un modelo DecisionTreeClassifier con los datos de entrenamiento (X_train, y_train).
- Prediga los valores de X_test.

Eval√∫e el modelo usando:

-Precisi√≥n (accuracy_score)
-Matriz de confusi√≥n (confusion_matrix)
-Reporte de clasificaci√≥n (classification_report)

evuelva un diccionario con:

-predicciones: Un array con las predicciones del modelo.
-accuracy: Un n√∫mero flotante con la precisi√≥n.
-matriz_confusion: Una matriz de confusi√≥n.
-reporte: Un string con el reporte de clasificaci√≥n.

Usa random_state=42 en DecisionTreeClassifier para reproducibilidad.
Prueba la funci√≥n con el dataset Iris, asegurando que el modelo tenga al menos 85% de precisi√≥n en los datos de prueba.


Precision (Precisi√≥n)

Qu√© mide: De todas las veces que el modelo dijo ‚Äúesta muestra es de la clase X‚Äù, cu√°ntas veces acert√≥.

Ejemplo:

Supongamos que el modelo predijo 10 flores como Setosa, pero solo 8 eran realmente Setosa.
Precision = 8 / 10 = 0.8 ‚Üí 80% de las predicciones para Setosa fueron correctas.

Recall (Sensibilidad o Exhaustividad)

Qu√© mide: De todas las muestras que realmente son de la clase X, cu√°ntas el modelo detect√≥ correctamente.

Ejemplo:

Hay 12 flores que son realmente Setosa. El modelo predijo correctamente 8 de ellas.
Recall = 8 / 12 ‚âà 0.67 ‚Üí Detect√≥ el 67% de las Setosa reales.

F1-score

Qu√© mide: Es un promedio que combina precision y recall, para dar una sola m√©trica balanceada.

Ejemplo:

Con el ejemplo anterior, precision = 0.8 y recall = 0.67
F1 ‚âà 2 * (0.8*0.67)/(0.8+0.67) ‚âà 0.73 ‚Üí Una sola medida que resume el desempe√±o.

4Ô∏è‚É£ Support (Soporte)

Qu√© mide: Cu√°ntas muestras reales hay de cada clase.
Ejemplo: Si hay 12 flores Setosa, 10 Versicolor y 8 Virginica, el support nos dice eso para cada clase.

üí° Resumiendo:

-Precision: ¬øDe todas mis predicciones, cu√°ntas fueron correctas?
-Recall: ¬øDe todas las muestras reales, cu√°ntas detect√© correctamente?
-F1-score: Balance entre precision y recall.
-Support: Cu√°ntas muestras de esa clase hab√≠a.

"""


import numpy as np
from sklearn.tree import DecisionTreeClassifier  # Para crear √°rboles de decisi√≥n
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Para evaluar el modelo
from sklearn.datasets import load_iris  # Dataset de ejemplo
from sklearn.model_selection import train_test_split  # Para dividir los datos en entrenamiento y prueba

def entrenar_y_evaluar_arbol(X_train, y_train, X_test, y_test):
    """
    Funci√≥n que entrena un √°rbol de decisi√≥n y eval√∫a su desempe√±o.

    Par√°metros:
    - X_train: caracter√≠sticas de entrenamiento (inputs)
    - y_train: etiquetas de entrenamiento (lo que queremos predecir)
    - X_test: caracter√≠sticas de prueba (inputs nuevos)
    - y_test: etiquetas reales de prueba (para comparar con las predicciones)
    
    Retorna un diccionario con:
    - predicciones: lo que el modelo predijo
    - accuracy: precisi√≥n del modelo (qu√© tan bien predijo)
    - matriz_confusion: muestra errores y aciertos por clase
    - reporte: m√©tricas m√°s detalladas por clase
    """

    # üëá Nombres de las clases del dataset Iris
    nombres_clases = ['Setosa', 'Versicolor', 'Virginica']

    # 1Ô∏è‚É£ Crear el modelo de √°rbol de decisi√≥n
    # random_state=42 asegura que los resultados sean reproducibles
    modelo = DecisionTreeClassifier(random_state=42)

    # 2Ô∏è‚É£ Entrenar el modelo usando los datos de entrenamiento
    # El modelo "aprende" la relaci√≥n entre X_train y y_train
    modelo.fit(X_train, y_train)

    # 3Ô∏è‚É£ Hacer predicciones sobre los datos de prueba
    predicciones = modelo.predict(X_test)  # Devuelve un array con las clases predichas

    # 4Ô∏è‚É£ Calcular m√©tricas para evaluar el desempe√±o del modelo
    accuracy = accuracy_score(y_test, predicciones)  # Qu√© porcentaje de predicciones fueron correctas
    matriz_confusion = confusion_matrix(y_test, predicciones)  # Muestra aciertos y errores por clase

    # 5Ô∏è‚É£ Crear un reporte m√°s detallado
    # Muestra precision, recall y f1-score por cada clase
    # labels=[0,1,2] indica los √≠ndices de las clases en y_test
    # target_names=nombres_clases reemplaza los n√∫meros por nombres legibles
    reporte = classification_report(
        y_test,
        predicciones,
        labels=[0, 1, 2],
        target_names=nombres_clases
    )

    # 6Ô∏è‚É£ Devolver todo en un diccionario para poder usarlo f√°cilmente
    return {
        'predicciones': np.array(predicciones),
        'accuracy': accuracy,
        'matriz_confusion': matriz_confusion,
        'reporte': reporte
    }


#üîπ Ejemplo de uso

# Cargar dataset Iris
iris = load_iris()
X = iris.data
y = iris.target

# Dividir en entrenamiento y prueba (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Llamar a la funci√≥n
resultados = entrenar_y_evaluar_arbol(X_train, y_train, X_test, y_test)

# Mostrar resultados
print("Precisi√≥n del modelo:", resultados["accuracy"])
print("Matriz de Confusi√≥n:\n", resultados["matriz_confusion"])
print("Reporte de Clasificaci√≥n:\n", resultados["reporte"])