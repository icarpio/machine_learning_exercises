"""
Las MÃ¡quinas de Vectores de Soporte (en inglÃ©s, Support Vector Machines â€” SVM) son uno de los algoritmos mÃ¡s potentes y populares del aprendizaje supervisado, 
especialmente para clasificaciÃ³n (aunque tambiÃ©n se pueden usar para regresiÃ³n y detecciÃ³n de outliers).

ğŸ§  Idea principal

Imagina que tienes dos grupos de puntos (clases) en un plano:

ğŸ”µ Clase A
ğŸ”´ Clase B

El objetivo del SVM es encontrar una lÃ­nea (o un hiperplano en dimensiones mayores) que separe ambas clases lo mejor posible.

Pero no cualquier lÃ­nea:
ğŸ‘‰ Se busca la que deja el mayor margen posible entre ambas clases.

Ese margen estÃ¡ definido por los puntos mÃ¡s cercanos al lÃ­mite, llamados vectores de soporte â€” de ahÃ­ el nombre del modelo.

ğŸ“ Concepto clave: el margen mÃ¡ximo

El SVM busca un hiperplano Ã³ptimo que:

Separe las clases correctamente (si es posible).
Maximice la distancia entre las clases (margen).
MatemÃ¡ticamente, el SVM resuelve un problema de optimizaciÃ³n convexa para maximizar ese margen bajo ciertas restricciones.

ğŸ”„ Cuando los datos no son lineales

En muchos casos, las clases no se pueden separar con una lÃ­nea recta.
Por ejemplo:

ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ
    ğŸ”´ğŸ”´ğŸ”´
    ğŸ”´ğŸ”´ğŸ”´


AhÃ­ entra en juego el truco del kernel (kernel trick).

âœ¨ El truco del kernel

Consiste en transformar los datos a un espacio de mayor dimensiÃ³n, donde sÃ­ se puedan separar linealmente, sin necesidad de calcular esa transformaciÃ³n explÃ­citamente.

Ejemplo:

En 2D no hay una lÃ­nea que separe bien los puntos.
En 3D (tras una transformaciÃ³n con un kernel), puede existir un plano separador perfecto.

Los kernels mÃ¡s usados:

linear: separa con una lÃ­nea recta.
poly: usa funciones polinomiales.
rbf o gaussian: transforma los datos con una funciÃ³n gaussiana (muy potente).
sigmoid: similar a una red neuronal.

"""



"""
Ejecicio SVM - MÃ¡quinas de vectores de soporte

Objetivo

El objetivo es implementar una funciÃ³n que:

Entrene un modelo de MÃ¡quina de Soporte Vectorial (SVM) usando SVC de sklearn.svm.
Realice predicciones en un conjunto de prueba.

EvalÃºe el modelo con las siguientes mÃ©tricas:

PrecisiÃ³n (accuracy_score).
Matriz de confusiÃ³n (confusion_matrix).
Reporte de clasificaciÃ³n (classification_report).
Devuelva los resultados en un diccionario.
Supervise la implementaciÃ³n con pruebas unitarias (unittest).



Instrucciones

Implementa una funciÃ³n llamada entrenar_y_evaluar_svm(X_train, y_train, X_test, y_test) que:
Entrene un modelo SVC(kernel='rbf', C=10.0, gamma='scale', random_state=42).
Prediga los valores de X_test.
Calcule las mÃ©tricas de evaluaciÃ³n mencionadas.

Devuelva un diccionario con:

"predicciones": Array de predicciones del modelo.
"accuracy": PrecisiÃ³n del modelo en los datos de prueba.
"matriz_confusion": Matriz de confusiÃ³n.
"reporte": Reporte de clasificaciÃ³n.

Usa el dataset de digits de sklearn.datasets, que contiene imÃ¡genes de nÃºmeros escritos a mano.
AsegÃºrate de que el modelo tenga al menos 90% de precisiÃ³n en los datos de prueba.

"""

"""
SVM (Support Vector Machine) es un modelo de clasificaciÃ³n supervisada que busca encontrar la frontera Ã³ptima (hiperplano) que separa las clases 
maximizando el margen entre los puntos mÃ¡s cercanos (vectores de soporte).
Usaremos un kernel RBF (radial basis function), que permite separar datos no lineales transformando el espacio de caracterÃ­sticas.
"""


from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

def entrenar_y_evaluar_svm(X_train, y_train, X_test, y_test):
    """
    Entrena y evalÃºa un modelo SVM con kernel RBF sobre los datos dados.

    ParÃ¡metros:
        X_train, y_train: Datos de entrenamiento
        X_test, y_test: Datos de prueba

    Retorna:
        Un diccionario con:
            - "predicciones": array de predicciones
            - "accuracy": precisiÃ³n del modelo
            - "matriz_confusion": matriz de confusiÃ³n
            - "reporte": reporte de clasificaciÃ³n
    """
    # 1ï¸âƒ£ Crear el modelo SVM
    modelo = SVC(kernel='rbf', C=10.0, gamma='scale', random_state=42)

    # 2ï¸âƒ£ Entrenar el modelo
    modelo.fit(X_train, y_train)

    # 3ï¸âƒ£ Hacer predicciones
    predicciones = modelo.predict(X_test)

    # 4ï¸âƒ£ Calcular mÃ©tricas
    accuracy = accuracy_score(y_test, predicciones)
    matriz = confusion_matrix(y_test, predicciones)
    reporte = classification_report(y_test, predicciones)

    # 5ï¸âƒ£ Devolver resultados
    resultados = {
        "predicciones": predicciones,
        "accuracy": accuracy,
        "matriz_confusion": matriz,
        "reporte": reporte
    }

    return resultados

#Ejemplo de uso

from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from solution import entrenar_y_evaluar_svm

# Cargar dataset
digits = load_digits()
X = digits.data
y = digits.target

# Dividir datos (80% entrenamiento, 20% prueba)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Entrenar y evaluar
resultados = entrenar_y_evaluar_svm(X_train, y_train, X_test, y_test)

# Mostrar resultados
print("PrecisiÃ³n del modelo:", resultados["accuracy"])
print("Matriz de ConfusiÃ³n:\n", resultados["matriz_confusion"])
print("Reporte de ClasificaciÃ³n:\n", resultados["reporte"])